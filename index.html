<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Natural Zoom Gesture on Frugal Devices</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Zoom Gesture</h1>
      <h2 class="project-tagline">Natural Zoom-in/Out gesture for frugal HMD's</h2>
      
     
    </section>

    <section class="main-content">
      <h3>
<a id="Welcome-to-ZoomGesture" class="anchor" href="#welcome-to-telepresence-roi" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Natural zoom Gesture</h3>

<p align ="justify">
  Hand gestures are a significant part of non-verbal communication
in our daily life. Air-gestures form a natural and intuitive interac-
tion mode with wearable head-mounted devices (HMDs). Applica-
tions of Gesture Recognition span wide area of applications includ-
ing Human Computer Interaction (HCI), sign languages, immersive
game technology to name a few. HMDs like Microsoft Hololens,
Meta2 etc. have incorporated wearable sensors at an additional ex-
pense to aid in interpretation of such gestures. This main contribu-
tions of the paper are (a) an on-board, real-time one-handed gesture
based interaction for Zoom-In / Zoom-Out manipulation of a scene
without incorporating any additional sensors and (b) Gabor filter
based pipeline for better hand segmentation that works on-device
than counterparts. The Zoom command is accomplished with natu-
ral gestures by pinching or stretching fingers, as is done with touch
screen devices. The underlying goal of this research is to extend
the interaction space around mobile devices for augmented reality,
like with Google Cardboard and Wearality, which are applications
where users can look at the live image from the devices camera. Our
proposed technique utilizes just RGB camera that is commonplace
on off-the-shelf mobile devices. Our algorithm robustly recognizes
Zoom in/out in-air gestures that were tested on different subjects
user varying light conditions. We demonstrate that our algorithm
runs in real-time on a resource-constrained android smartphones.
Finally, we report results from preliminary user evaluationsâ€“ (a)
confusion matrix for classification (b) IoU metric for hand segmen-
tation evaluation, and (c) computation time for Zoom gesture. We
discuss the advantages and limitations of our proposed method with
deep learning based methods such as the Faster RCNN. Subjective
metrics such as the comfort level in subjects while performing the
gesture is reported.</p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/ondevicezoomGesture/zOOmgesture/blob/master/img/Zoomflow.png?raw=true?" alt="Proposed Method " height = "600" width="550" align = "middle"></p>

<p align="justify"> We  present an  approach  for  marker-less  and  real-time touch-less gestures for Zoom in/out on wearables in FPV by tracking
  the movement of Thumb and Index finger. Our  approach  is  particularly  suitable  as  most  of  the smartphones available in 
  the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: 
  (i) hand segmentaion, (ii) stable hand detection, (iii) left/right hand detection (iv) thumb and index finger segmentation 
  (v) tracking & analysing thumb & index finger movement. First we average out similar texture region and highlight edges between different texture. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. Left/Right hand detection is done by determing the slope of hand, because hand orientation will be of fix type in Google Cardboard/HMD.
  If Left hand is detected, right most point on hand countour is extreme point. Similarly, if right hand is detected, left most point on 
  hand countour is extreme point. After the left/right hand is detected, center of extreme point & centroid of contour is calculated.
  The hand region towards centroid of contour from center is removed. Now of the largest two blob one is thumb & other is index
  finger. The distance between fingres is analysed to classify Zoom gesture. We conducted experiments which  demonstrates  that our method of Zoom 
  Gesture, work in real time, on device in complex background. </p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/hI6QFWPn9X0" frameborder="0" allowfullscreen></iframe>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ondevicezoomGesture/zOOmgesture">zoom Gesture</a> is maintained by <a href="https://github.com/ondevicezoomGesture">ondevicezoomGesture</a>.</span>

      </footer>

    </section>

  </body>
</html>
